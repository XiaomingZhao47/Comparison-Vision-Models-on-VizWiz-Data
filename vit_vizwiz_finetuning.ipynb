{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) Fine-tuning for VizWiz\n",
    "\n",
    "This notebook implements ViT fine-tuning for the VizWiz dataset with focus on:\n",
    "1. Image quality classification (answerable vs unanswerable)\n",
    "2. Question type classification\n",
    "3. Degradation type detection (blur, darkness, poor framing)\n",
    "\n",
    "Since ViT cannot directly do VQA, we'll train it as a multi-task classifier to:\n",
    "- Predict if an image is answerable\n",
    "- Classify question types (OCR-like, color, object, etc.)\n",
    "- Detect image quality issues\n",
    "\n",
    "# Quick Start\n",
    "```\n",
    "jupyter lab --no-browser\n",
    "http://127.0.0.1:8888/lab 123\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render Widget, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render Widget, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import wandb\n",
    "wandb.init(project=\"vit-vizwiz\", name=\"experiment-1\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data keys: dict_keys(['info', 'images', 'annotations'])\n",
      "Train images: 23431\n",
      "Train annotations: 117155\n",
      "\n",
      "Val images: 7750\n",
      "Val annotations: 38750\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "def load_vizwiz_annotations(split='train'):\n",
    "    \"\"\"\n",
    "    Load VizWiz annotations.\n",
    "    Note: Your current files seem to have limited info.\n",
    "    You may need to download the full dataset from:\n",
    "    https://vizwiz.org/tasks-and-datasets/vqa/\n",
    "    \"\"\"\n",
    "    with open(f'data/annotations/{split}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load all splits\n",
    "train_data = load_vizwiz_annotations('train')\n",
    "val_data = load_vizwiz_annotations('val')\n",
    "\n",
    "print(\"Train data keys:\", train_data.keys())\n",
    "print(f\"Train images: {len(train_data['images'])}\")\n",
    "print(f\"Train annotations: {len(train_data['annotations'])}\")\n",
    "print(f\"\\nVal images: {len(val_data['images'])}\")\n",
    "print(f\"Val annotations: {len(val_data['annotations'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Type Classifier (Heuristic-based)\n",
    "\n",
    "Based on your paper's Table 2, we'll classify questions into buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does this label say?                -> OCR_LIKE\n",
      "What color is this shirt?                -> COLOR\n",
      "How many bottles are there?              -> COUNT\n",
      "What is on the left?                     -> DIRECTION\n",
      "What time is it?                         -> TIME\n",
      "What is this?                            -> OTHER\n"
     ]
    }
   ],
   "source": [
    "def classify_question_type(question):\n",
    "    \"\"\"\n",
    "    Classify question into types based on keywords:\n",
    "    - OCR_LIKE: text reading questions\n",
    "    - COLOR: color-related questions\n",
    "    - COUNT: counting questions\n",
    "    - DIRECTION: directional questions\n",
    "    - TIME: time-related questions\n",
    "    - OTHER: everything else\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # OCR-related keywords\n",
    "    ocr_keywords = ['read', 'say', 'text', 'label', 'written', 'writing', \n",
    "                    'words', 'screen', 'display', 'says', 'does this say']\n",
    "    if any(keyword in question_lower for keyword in ocr_keywords):\n",
    "        return 'OCR_LIKE'\n",
    "    \n",
    "    # Color keywords\n",
    "    color_keywords = ['color', 'colour', 'what color']\n",
    "    if any(keyword in question_lower for keyword in color_keywords):\n",
    "        return 'COLOR'\n",
    "    \n",
    "    # Count keywords\n",
    "    count_keywords = ['how many', 'count', 'number of']\n",
    "    if any(keyword in question_lower for keyword in count_keywords):\n",
    "        return 'COUNT'\n",
    "    \n",
    "    # Direction keywords\n",
    "    direction_keywords = ['left', 'right', 'top', 'bottom', 'front', 'back',\n",
    "                         'above', 'below', 'which side']\n",
    "    if any(keyword in question_lower for keyword in direction_keywords):\n",
    "        return 'DIRECTION'\n",
    "    \n",
    "    # Time keywords\n",
    "    time_keywords = ['time', 'clock', 'hour', 'minute']\n",
    "    if any(keyword in question_lower for keyword in time_keywords):\n",
    "        return 'TIME'\n",
    "    \n",
    "    return 'OTHER'\n",
    "\n",
    "# Test the classifier\n",
    "test_questions = [\n",
    "    \"What does this label say?\",\n",
    "    \"What color is this shirt?\",\n",
    "    \"How many bottles are there?\",\n",
    "    \"What is on the left?\",\n",
    "    \"What time is it?\",\n",
    "    \"What is this?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"{q:40s} -> {classify_question_type(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Quality Detection\n",
    "\n",
    "Detect blur, darkness, and poor contrast as mentioned in your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_blur(image_path, threshold=100):\n",
    "    \"\"\"Detect if image is blurry using Laplacian variance.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return variance < threshold\n",
    "\n",
    "def detect_darkness(image_path, threshold=50):\n",
    "    \"\"\"Detect if image is too dark.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    return mean_brightness < threshold\n",
    "\n",
    "def detect_low_contrast(image_path, threshold=30):\n",
    "    \"\"\"Detect if image has low contrast.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast = gray.std()\n",
    "    return contrast < threshold\n",
    "\n",
    "def get_image_quality_features(image_path):\n",
    "    \"\"\"Get all quality features for an image.\"\"\"\n",
    "    return {\n",
    "        'is_blurry': detect_blur(image_path),\n",
    "        'is_dark': detect_darkness(image_path),\n",
    "        'is_low_contrast': detect_low_contrast(image_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Preparation\n",
    "\n",
    "\n",
    "Dataset from: https://vizwiz.org/tasks-and-datasets/vqa/\n",
    "\n",
    "create a dataset for answerability classification as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor loaded\n"
     ]
    }
   ],
   "source": [
    "class VizWizAnswerabilityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for predicting if an image/question pair is answerable.\n",
    "    binary classification task for ViT \n",
    "    \"\"\"\n",
    "    def __init__(self, annotations, images_dir, feature_extractor, max_samples=None):\n",
    "        self.annotations = annotations['annotations']\n",
    "        self.images = {img['id']: img for img in annotations['images']}\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        if max_samples:\n",
    "            self.annotations = self.annotations[:max_samples]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        image_info = self.images[ann['image_id']]\n",
    "        \n",
    "        image_path = self.images_dir / image_info['file_name']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        \n",
    "        # Label: 1 if answerable (not rejected), 0 if unanswerable\n",
    "        label = 0 if ann.get('is_rejected', False) else 1\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# eature extractor\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "print(\"Feature extractor loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Setup\n",
    "\n",
    "We'll fine-tune ViT for binary classification (answerable vs unanswerable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Number of parameters: 85,800,194\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification: answerable vs unanswerable\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Datasets\n",
    "\n",
    "**IMPORTANT**: Update the `images_dir` path to where your VizWiz images are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 100\n",
      "Val dataset size: 50\n",
      "\n",
      "Sample pixel_values shape: torch.Size([3, 224, 224])\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES_DIR = \"data/train\"  \n",
    "VAL_IMAGES_DIR = \"data/val\"      \n",
    "\n",
    "# create datasets \n",
    "train_dataset = VizWizAnswerabilityDataset(\n",
    "    train_data,\n",
    "    TRAIN_IMAGES_DIR,\n",
    "    feature_extractor,\n",
    "    max_samples=100  # trial, adjust as needed\n",
    ")\n",
    "\n",
    "val_dataset = VizWizAnswerabilityDataset(\n",
    "    val_data,\n",
    "    VAL_IMAGES_DIR,\n",
    "    feature_extractor,\n",
    "    max_samples=50  # trial, adjust as needed\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Sample label: {sample['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy metrics.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/vit_vizwiz_answerability\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    report_to=\"wandb\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  eval_loss: nan\n",
      "  eval_accuracy: 0.0000\n",
      "  eval_runtime: 0.8668\n",
      "  eval_samples_per_second: 57.6860\n",
      "  eval_steps_per_second: 2.3070\n",
      "  epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/vit_vizwiz_finetuned\")\n",
    "feature_extractor.save_pretrained(\"./models/vit_vizwiz_finetuned\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Detailed Analysis (Per Question Type)\n",
    "\n",
    "This analyzes performance by question type as mentioned in your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# overview\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    target_names=['Unanswerable', 'Answerable']\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ViT-VizWiz)",
   "language": "python",
   "name": "vit_vizwiz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
