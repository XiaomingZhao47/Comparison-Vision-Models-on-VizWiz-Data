{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) Fine-tuning for VizWiz\n",
    "\n",
    "This notebook implements ViT fine-tuning for the VizWiz dataset with focus on:\n",
    "1. Image quality classification (answerable vs unanswerable)\n",
    "2. Question type classification\n",
    "3. Degradation type detection (blur, darkness, poor framing)\n",
    "\n",
    "Since ViT cannot directly do VQA, we'll train it as a multi-task classifier to:\n",
    "- Predict if an image is answerable\n",
    "- Classify question types (OCR-like, color, object, etc.)\n",
    "- Detect image quality issues\n",
    "\n",
    "# Quick Start\n",
    "```\n",
    "jupyter lab --no-browser\n",
    "http://127.0.0.1:8888/lab 123\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoming-zhao/Comparison-Vision-Models-on-VizWiz-Data/vit_vizwiz_env/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/xiaoming-zhao/Comparison-Vision-Models-on-VizWiz-Data/vit_vizwiz_env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/xiaoming-zhao/Comparison-Vision-Models-on-VizWiz-Data/vit_vizwiz_env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjelly47\u001b[0m (\u001b[33mjelly47-dartmouth\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import wandb\n",
    "wandb.init(project=\"vit-vizwiz\", name=\"experiment-1\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data keys: dict_keys(['info', 'images', 'annotations'])\n",
      "Train images: 23431\n",
      "Train annotations: 117155\n",
      "\n",
      "Val images: 7750\n",
      "Val annotations: 38750\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "def load_vizwiz_annotations(split='train'):\n",
    "    \"\"\"\n",
    "    Load VizWiz annotations.\n",
    "    Note: Your current files seem to have limited info.\n",
    "    You may need to download the full dataset from:\n",
    "    https://vizwiz.org/tasks-and-datasets/vqa/\n",
    "    \"\"\"\n",
    "    with open(f'data/annotations/{split}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load all splits\n",
    "train_data = load_vizwiz_annotations('train')\n",
    "val_data = load_vizwiz_annotations('val')\n",
    "\n",
    "print(\"Train data keys:\", train_data.keys())\n",
    "print(f\"Train images: {len(train_data['images'])}\")\n",
    "print(f\"Train annotations: {len(train_data['annotations'])}\")\n",
    "print(f\"\\nVal images: {len(val_data['images'])}\")\n",
    "print(f\"Val annotations: {len(val_data['annotations'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Type Classifier (Heuristic-based)\n",
    "\n",
    "Based on your paper's Table 2, we'll classify questions into buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does this label say?                -> OCR_LIKE\n",
      "What color is this shirt?                -> COLOR\n",
      "How many bottles are there?              -> COUNT\n",
      "What is on the left?                     -> DIRECTION\n",
      "What time is it?                         -> TIME\n",
      "What is this?                            -> OTHER\n"
     ]
    }
   ],
   "source": [
    "def classify_question_type(question):\n",
    "    \"\"\"\n",
    "    Classify question into types based on keywords:\n",
    "    - OCR_LIKE: text reading questions\n",
    "    - COLOR: color-related questions\n",
    "    - COUNT: counting questions\n",
    "    - DIRECTION: directional questions\n",
    "    - TIME: time-related questions\n",
    "    - OTHER: everything else\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # OCR-related keywords\n",
    "    ocr_keywords = ['read', 'say', 'text', 'label', 'written', 'writing', \n",
    "                    'words', 'screen', 'display', 'says', 'does this say']\n",
    "    if any(keyword in question_lower for keyword in ocr_keywords):\n",
    "        return 'OCR_LIKE'\n",
    "    \n",
    "    # Color keywords\n",
    "    color_keywords = ['color', 'colour', 'what color']\n",
    "    if any(keyword in question_lower for keyword in color_keywords):\n",
    "        return 'COLOR'\n",
    "    \n",
    "    # Count keywords\n",
    "    count_keywords = ['how many', 'count', 'number of']\n",
    "    if any(keyword in question_lower for keyword in count_keywords):\n",
    "        return 'COUNT'\n",
    "    \n",
    "    # Direction keywords\n",
    "    direction_keywords = ['left', 'right', 'top', 'bottom', 'front', 'back',\n",
    "                         'above', 'below', 'which side']\n",
    "    if any(keyword in question_lower for keyword in direction_keywords):\n",
    "        return 'DIRECTION'\n",
    "    \n",
    "    # Time keywords\n",
    "    time_keywords = ['time', 'clock', 'hour', 'minute']\n",
    "    if any(keyword in question_lower for keyword in time_keywords):\n",
    "        return 'TIME'\n",
    "    \n",
    "    return 'OTHER'\n",
    "\n",
    "# Test the classifier\n",
    "test_questions = [\n",
    "    \"What does this label say?\",\n",
    "    \"What color is this shirt?\",\n",
    "    \"How many bottles are there?\",\n",
    "    \"What is on the left?\",\n",
    "    \"What time is it?\",\n",
    "    \"What is this?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"{q:40s} -> {classify_question_type(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Quality Detection\n",
    "\n",
    "Detect blur, darkness, and poor contrast as mentioned in your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_blur(image_path, threshold=100):\n",
    "    \"\"\"Detect if image is blurry using Laplacian variance.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return variance < threshold\n",
    "\n",
    "def detect_darkness(image_path, threshold=50):\n",
    "    \"\"\"Detect if image is too dark.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    return mean_brightness < threshold\n",
    "\n",
    "def detect_low_contrast(image_path, threshold=30):\n",
    "    \"\"\"Detect if image has low contrast.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast = gray.std()\n",
    "    return contrast < threshold\n",
    "\n",
    "def get_image_quality_features(image_path):\n",
    "    \"\"\"Get all quality features for an image.\"\"\"\n",
    "    return {\n",
    "        'is_blurry': detect_blur(image_path),\n",
    "        'is_dark': detect_darkness(image_path),\n",
    "        'is_low_contrast': detect_low_contrast(image_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Preparation\n",
    "\n",
    "\n",
    "Dataset from: https://vizwiz.org/tasks-and-datasets/vqa/\n",
    "\n",
    "create a dataset for answerability classification as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor loaded\n"
     ]
    }
   ],
   "source": [
    "class VizWizAnswerabilityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for predicting if an image/question pair is answerable.\n",
    "    binary classification task for ViT \n",
    "    \"\"\"\n",
    "    def __init__(self, annotations, images_dir, feature_extractor, max_samples=None):\n",
    "        self.annotations = annotations['annotations']\n",
    "        self.images = {img['id']: img for img in annotations['images']}\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        if max_samples:\n",
    "            self.annotations = self.annotations[:max_samples]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        image_info = self.images[ann['image_id']]\n",
    "        \n",
    "        image_path = self.images_dir / image_info['file_name']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        \n",
    "        # Label: 1 if answerable (not rejected), 0 if unanswerable\n",
    "        label = 0 if ann.get('is_rejected', False) else 1\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# eature extractor\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "print(\"Feature extractor loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Setup\n",
    "\n",
    "We'll fine-tune ViT for binary classification (answerable vs unanswerable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Number of parameters: 85,800,194\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification: answerable vs unanswerable\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Datasets\n",
    "\n",
    "**IMPORTANT**: Update the `images_dir` path to where your VizWiz images are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1000\n",
      "Val dataset size: 500\n",
      "\n",
      "Sample pixel_values shape: torch.Size([3, 224, 224])\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES_DIR = \"data/train\"  \n",
    "VAL_IMAGES_DIR = \"data/val\"      \n",
    "\n",
    "# create datasets \n",
    "train_dataset = VizWizAnswerabilityDataset(\n",
    "    train_data,\n",
    "    TRAIN_IMAGES_DIR,\n",
    "    feature_extractor,\n",
    "    max_samples=1000  # adjust as needed\n",
    ")\n",
    "\n",
    "val_dataset = VizWizAnswerabilityDataset(\n",
    "    val_data,\n",
    "    VAL_IMAGES_DIR,\n",
    "    feature_extractor,\n",
    "    max_samples=500  # adjust as needed\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Sample label: {sample['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "  Unanswerable (0): 25 (2.5%)\n",
      "  Answerable (1): 975 (97.5%)\n",
      "\n",
      "Class weights:\n",
      "  Unanswerable: 50.00x\n",
      "  Answerable: 1.00x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support  # â† Added this\n",
    "from collections import Counter\n",
    "\n",
    "# Get training labels\n",
    "train_labels = [0 if ann.get('is_rejected', False) else 1 \n",
    "                for ann in train_data['annotations'][:1000]]  # Your max_samples\n",
    "\n",
    "class_counts = Counter(train_labels)\n",
    "print(\"Training set distribution:\")\n",
    "print(f\"  Unanswerable (0): {class_counts[0]} ({class_counts[0]/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  Answerable (1): {class_counts[1]} ({class_counts[1]/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "# total = len(train_labels)\n",
    "# class_weights = [\n",
    "#     np.sqrt(total / class_counts[0]) if class_counts[0] > 0 else 1.0,\n",
    "#     np.sqrt(total / class_counts[1]) if class_counts[1] > 0 else 1.0\n",
    "# ]\n",
    "\n",
    "# weight_sum = sum(class_weights)\n",
    "# class_weights = [w / weight_sum * 2 for w in class_weights]  # Scale to average of 1\n",
    "\n",
    "class_weights = [\n",
    "    50.0,  # Unanswerable - make it 20x more important\n",
    "    1.0    # Answerable - baseline\n",
    "]\n",
    "\n",
    "print(f\"\\nClass weights:\")\n",
    "print(f\"  Unanswerable: {class_weights[0]:.2f}x\")\n",
    "print(f\"  Answerable: {class_weights[1]:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive metrics.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, labels=[0, 1], zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro average\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'unanswerable_precision': precision[0],\n",
    "        'unanswerable_recall': recall[0],\n",
    "        'unanswerable_f1': f1[0],\n",
    "        'answerable_precision': precision[1],\n",
    "        'answerable_recall': recall[1],\n",
    "        'answerable_f1': f1[1],\n",
    "        'f1_macro': f1_macro,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Weighted CrossEntropy loss\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor(class_weights, device=model.device, dtype=torch.float)\n",
    "        )\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/vit_vizwiz_weighted\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",  \n",
    "    greater_is_better=True,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    report_to=[\"wandb\", \"tensorboard\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedTrainer(  \n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 01:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Unanswerable Precision</th>\n",
       "      <th>Unanswerable Recall</th>\n",
       "      <th>Unanswerable F1</th>\n",
       "      <th>Answerable Precision</th>\n",
       "      <th>Answerable Recall</th>\n",
       "      <th>Answerable F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.730718</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>0.493414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./models/vit_vizwiz_weighted/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render Widget, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  eval_loss: 0.7307\n",
      "  eval_accuracy: 0.9740\n",
      "  eval_unanswerable_precision: 0.0000\n",
      "  eval_unanswerable_recall: 0.0000\n",
      "  eval_unanswerable_f1: 0.0000\n",
      "  eval_answerable_precision: 0.9740\n",
      "  eval_answerable_recall: 1.0000\n",
      "  eval_answerable_f1: 0.9868\n",
      "  eval_f1_macro: 0.4934\n",
      "  eval_runtime: 6.8842\n",
      "  eval_samples_per_second: 72.6300\n",
      "  eval_steps_per_second: 2.3240\n",
      "  epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"vit-vizwiz\",\n",
    "    name=\"evaluation-only\",\n",
    "    config={\"mode\": \"evaluation\"}\n",
    ")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/vit_vizwiz_finetuned\")\n",
    "feature_extractor.save_pretrained(\"./models/vit_vizwiz_finetuned\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analysis\n",
    "\n",
    "This analyzes performance by question type as mentioned in your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Unanswerable       0.00      0.00      0.00        13\n",
      "  Answerable       0.97      1.00      0.99       487\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.49      0.50      0.49       500\n",
      "weighted avg       0.95      0.97      0.96       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# overview\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    target_names=['Unanswerable', 'Answerable']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test with loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\"./models/vit_vizwiz_finetuned\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"./models/vit_vizwiz_finetuned\")\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open(\"test.jpg\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ViT-VizWiz)",
   "language": "python",
   "name": "vit_vizwiz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
