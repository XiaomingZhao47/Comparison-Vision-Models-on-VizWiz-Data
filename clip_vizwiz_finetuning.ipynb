{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c017e2",
   "metadata": {},
   "source": [
    "# CLIP Fine-tuning for VizWiz\n",
    "\n",
    "This notebook adapts your previous ViT fine-tuning pipeline to use **CLIP** instead of ViT.\n",
    "\n",
    "We keep the overall structure similar:\n",
    "1. Load VizWiz annotations\n",
    "2. Build a binary classification task: *answerable vs unanswerable*\n",
    "3. Prepare a PyTorch `Dataset` + `DataLoader`\n",
    "4. Fine-tune CLIPâ€™s **vision encoder + classification head**\n",
    "5. Evaluate and save the model\n",
    "\n",
    "> Note: We use HuggingFace `openai/clip-vit-base-patch32` and a custom PyTorch training loop (Trainer from HF is mainly for Encoder/Decoder or causal models; CLIP here is easier with a manual loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cdaafff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef0018",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Analysis\n",
    "\n",
    "We assume you have **VizWiz JSON annotations** in `data/annotations/{train,val}.json` and images in `data/train`, `data/val` (same as your ViT notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70a7903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "def load_vizwiz_annotations(split='train'):\n",
    "    \"\"\"Load VizWiz annotations from data/Annotations/{split}.json\"\"\"\n",
    "    with open(f'data/Annotations/{split}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load all splits\n",
    "train_data = load_vizwiz_annotations('train')\n",
    "val_data = load_vizwiz_annotations('val')\n",
    "\n",
    "# print(\"Train data keys:\", train_data.keys())\n",
    "# print(f\"Train images: {len(train_data['images'])}\")\n",
    "# print(f\"Train annotations: {len(train_data['annotations'])}\")\n",
    "# print(f\"\\nVal images: {len(val_data['images'])}\")\n",
    "# print(f\"Val annotations: {len(val_data['annotations'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef2a09",
   "metadata": {},
   "source": [
    "## 3. Question Type Classifier (Heuristic-based)\n",
    "\n",
    "We keep your heuristic **question type** function for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a88b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does this label say?                -> OCR_LIKE\n",
      "What color is this shirt?                -> COLOR\n",
      "How many bottles are there?              -> COUNT\n",
      "What is on the left?                     -> DIRECTION\n",
      "What time is it?                         -> TIME\n",
      "What is this?                            -> OTHER\n"
     ]
    }
   ],
   "source": [
    "def classify_question_type(question):\n",
    "    \"\"\"Classify question into coarse types using keywords.\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # OCR-related keywords\n",
    "    ocr_keywords = ['read', 'say', 'text', 'label', 'written', 'writing', \n",
    "                    'words', 'screen', 'display', 'says', 'does this say']\n",
    "    if any(keyword in question_lower for keyword in ocr_keywords):\n",
    "        return 'OCR_LIKE'\n",
    "    \n",
    "    # Color keywords\n",
    "    color_keywords = ['color', 'colour', 'what color']\n",
    "    if any(keyword in question_lower for keyword in color_keywords):\n",
    "        return 'COLOR'\n",
    "    \n",
    "    # Count keywords\n",
    "    count_keywords = ['how many', 'count', 'number of']\n",
    "    if any(keyword in question_lower for keyword in count_keywords):\n",
    "        return 'COUNT'\n",
    "    \n",
    "    # Direction keywords\n",
    "    direction_keywords = ['left', 'right', 'top', 'bottom', 'front', 'back',\n",
    "                         'above', 'below', 'which side']\n",
    "    if any(keyword in question_lower for keyword in direction_keywords):\n",
    "        return 'DIRECTION'\n",
    "    \n",
    "    # Time keywords\n",
    "    time_keywords = ['time', 'clock', 'hour', 'minute']\n",
    "    if any(keyword in question_lower for keyword in time_keywords):\n",
    "        return 'TIME'\n",
    "    \n",
    "    return 'OTHER'\n",
    "\n",
    "# Quick sanity check\n",
    "test_questions = [\n",
    "    \"What does this label say?\",\n",
    "    \"What color is this shirt?\",\n",
    "    \"How many bottles are there?\",\n",
    "    \"What is on the left?\",\n",
    "    \"What time is it?\",\n",
    "    \"What is this?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"{q:40s} -> {classify_question_type(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd271c9",
   "metadata": {},
   "source": [
    "## 4. Image Quality Detection (Optional Features)\n",
    "\n",
    "We reuse your blur/darkness/contrast detection utilities (can be used later for analysis or as extra signals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "786372a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_blur(image_path, threshold=100):\n",
    "    \"\"\"Detect if image is blurry using Laplacian variance.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return variance < threshold\n",
    "\n",
    "def detect_darkness(image_path, threshold=50):\n",
    "    \"\"\"Detect if image is too dark.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    return mean_brightness < threshold\n",
    "\n",
    "def detect_low_contrast(image_path, threshold=30):\n",
    "    \"\"\"Detect if image has low contrast.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast = gray.std()\n",
    "    return contrast < threshold\n",
    "\n",
    "def get_image_quality_features(image_path):\n",
    "    \"\"\"Get all quality features for an image.\"\"\"\n",
    "    return {\n",
    "        'is_blurry': detect_blur(image_path),\n",
    "        'is_dark': detect_darkness(image_path),\n",
    "        'is_low_contrast': detect_low_contrast(image_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e2a44",
   "metadata": {},
   "source": [
    "## 5. Dataset Preparation for Answerability Classification (CLIP Version)\n",
    "\n",
    "We now create a **CLIP-based Dataset** for binary classification *(answerable vs unanswerable)*.\n",
    "\n",
    "- CLIP Processor handles image resizing/normalization.\n",
    "- Label = 1 if not rejected, 0 if `is_rejected == True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea543ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP processor loaded\n"
     ]
    }
   ],
   "source": [
    "class VizWizAnswerabilityCLIPDataset(Dataset):\n",
    "    def __init__(self, samples, images_dir, processor, max_samples=None):\n",
    "        self.samples = samples\n",
    "        if max_samples is not None:\n",
    "            self.samples = self.samples[:max_samples]\n",
    "        \n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        \n",
    "        # ---------- image ----------\n",
    "        image_name = item.get(\"image\")  # e.g. \"VizWiz_val_00001234.jpg\"\n",
    "        image_path = self.images_dir / image_name\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception:\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        # ---------- CLIP processor ----------\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # ---------- label ----------\n",
    "        if \"answerable\" in item:\n",
    "            label = int(item[\"answerable\"])\n",
    "        else:\n",
    "            label = 0 if item.get(\"is_rejected\", False) else 1\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"question\": item.get(\"question\", \"\"),\n",
    "            \"image_path\": str(image_path),\n",
    "        }\n",
    "\n",
    "# Initialize CLIP processor\n",
    "clip_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(clip_name)\n",
    "print(\"CLIP processor loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb900c",
   "metadata": {},
   "source": [
    "## 6. CLIP Model Setup\n",
    "\n",
    "We will:\n",
    "- Load `CLIPModel`.\n",
    "- Take its **vision encoder** output.\n",
    "- Add a small **classification head (Linear)** on top.\n",
    "- Fine-tune only the head (or optionally unfreeze some CLIP layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b00c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Freeze mode = partial: unfreeze vision encoder last 8 layers (layer 4~11) + visual_projection + classifier.\n",
      "Total params: 151,804,675\n",
      "Trainable params: 57,623,554\n"
     ]
    }
   ],
   "source": [
    "class CLIPAnswerabilityClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"openai/clip-vit-base-patch32\",\n",
    "        num_labels=2,\n",
    "        freeze_mode=\"partial\",          # \"head_only\" | \"partial\" | \"full\"\n",
    "        num_unfrozen_vision_layers=4    # only used when freeze_mode == \"partial\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(base_model_name)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # get_image_features output dimension (usually 512 for ViT-B/32)\n",
    "        embed_dim = self.clip.visual_projection.out_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_labels)\n",
    "        )\n",
    "\n",
    "        # --------- Freezing strategy ---------\n",
    "        # First freeze all CLIP parameters\n",
    "        for p in self.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        if freeze_mode == \"head_only\":\n",
    "            msg = \"ðŸ‘‰ Freeze mode = head_only: only classifier head is trainable.\"\n",
    "\n",
    "        elif freeze_mode == \"partial\":\n",
    "            # Partially unfreeze: last N vision encoder layers + visual_projection\n",
    "            vision_layers = list(self.clip.vision_model.encoder.layers)\n",
    "            n_layers = len(vision_layers)\n",
    "            start_idx = max(0, n_layers - num_unfrozen_vision_layers)\n",
    "\n",
    "            for i in range(start_idx, n_layers):\n",
    "                for p in vision_layers[i].parameters():\n",
    "                    p.requires_grad = True\n",
    "            for p in self.clip.visual_projection.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            msg = (\n",
    "                f\"ðŸ‘‰ Freeze mode = partial: unfreeze vision encoder last \"\n",
    "                f\"{num_unfrozen_vision_layers} layers (layer {start_idx}~{n_layers-1}) \"\n",
    "                f\"+ visual_projection + classifier.\"\n",
    "            )\n",
    "\n",
    "        elif freeze_mode == \"full\":\n",
    "            # Fully unfreeze all CLIP parameters\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = True\n",
    "            msg = \"ðŸ‘‰ Freeze mode = full: CLIP backbone fully trainable.\"\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown freeze_mode: {freeze_mode}\")\n",
    "\n",
    "        # Classifier head should always be trainable\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        print(msg)\n",
    "\n",
    "        # Print parameter stats\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Total params: {total_params:,}\")\n",
    "        print(f\"Trainable params: {trainable_params:,}\")\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        \"\"\"\n",
    "        pixel_values: [B, 3, H, W]\n",
    "        returns: (loss, logits) if labels is not None\n",
    "                 (None, logits) otherwise\n",
    "        \"\"\"\n",
    "        # get_image_features â†’ [B, D], already normalized\n",
    "        image_embeds = self.clip.get_image_features(pixel_values=pixel_values)  # [B, D]\n",
    "\n",
    "        logits = self.classifier(image_embeds)  # [B, num_labels]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "\n",
    "        return loss, logits\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CLIPAnswerabilityClassifier(\n",
    "    base_model_name=\"openai/clip-vit-base-patch32\",\n",
    "    num_labels=2,\n",
    "    freeze_mode=\"partial\",         # \"head_only\" / \"partial\" / \"full\"\n",
    "    num_unfrozen_vision_layers=8   # try 2 / 4 / 6 etc.\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604f849",
   "metadata": {},
   "source": [
    "## 7. Create Datasets & Dataloaders\n",
    "\n",
    "Update the `TRAIN_IMAGES_DIR` and `VAL_IMAGES_DIR` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2b5ae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3000\n",
      "Val dataset size: 1000\n",
      "\n",
      "Sample pixel_values shape: torch.Size([3, 224, 224])\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES_DIR = \"data/train\"\n",
    "VAL_IMAGES_DIR = \"data/val\"\n",
    "\n",
    "train_dataset = VizWizAnswerabilityCLIPDataset(\n",
    "    train_data,\n",
    "    TRAIN_IMAGES_DIR,\n",
    "    processor,\n",
    "    max_samples=3000   # trial; increase when things work\n",
    ")\n",
    "\n",
    "val_dataset = VizWizAnswerabilityCLIPDataset(\n",
    "    val_data,\n",
    "    VAL_IMAGES_DIR,\n",
    "    processor,\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Sample label: {sample['labels']}\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([b['pixel_values'] for b in batch])\n",
    "    labels = torch.stack([b['labels'] for b in batch])\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60ca3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable backbone params: 57096192\n",
      "Trainable head params: 527362\n"
     ]
    }
   ],
   "source": [
    "backbone_params = []\n",
    "head_params = []\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith(\"classifier.\"):\n",
    "        head_params.append(p)\n",
    "    else:\n",
    "        backbone_params.append(p)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": backbone_params, \"lr\": 3e-5},  # smaller lr for backbone\n",
    "        {\"params\": head_params, \"lr\": 5e-5},      # larger lr for classifier head\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(\"Trainable backbone params:\", sum(p.numel() for p in backbone_params))\n",
    "print(\"Trainable head params:\", sum(p.numel() for p in head_params))\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss, logits = model(pixel_values=pixel_values, labels=None)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc, np.array(all_labels), np.array(all_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14963be6",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "We implement a standard PyTorch training loop (similar spirit to your Trainer setup), and compute accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffe0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [01:34<00:00,  1.99it/s, loss=0.4188]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg train loss = 0.4488, val acc = 0.6280\n",
      " New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [01:10<00:00,  2.68it/s, loss=0.1299]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg train loss = 0.2876, val acc = 0.6340\n",
      " New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [01:12<00:00,  2.59it/s, loss=0.4338]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg train loss = 0.1966, val acc = 0.6260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [01:25<00:00,  2.19it/s, loss=0.0338]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg train loss = 0.0963, val acc = 0.6380\n",
      " New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [01:13<00:00,  2.56it/s, loss=0.0258]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg train loss = 0.0344, val acc = 0.6420\n",
      " New best model saved.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 13\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    val_acc, _, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch}: avg train loss = {avg_loss:.4f}, val acc = {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"clip_vizwiz_answerability_best_partial.pth\")\n",
    "        print(\" New best model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9ed09",
   "metadata": {},
   "source": [
    "## 9. Evaluation & Report\n",
    "\n",
    "After training, we run a final evaluation and print a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6d01307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Accuracy: 0.5900\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Unanswerable       0.55      0.57      0.56       231\n",
      "  Answerable       0.62      0.61      0.61       269\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.59      0.59      0.59       500\n",
      "weighted avg       0.59      0.59      0.59       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "val_acc, true_labels, pred_labels = evaluate(model, val_loader, device)\n",
    "print(f\"\\nFinal Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    target_names=['Unanswerable', 'Answerable']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e608c1",
   "metadata": {},
   "source": [
    "## 10. Save Model & Processor\n",
    "\n",
    "We save the fine-tuned classifier weights together with the CLIP processor configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"./models/clip_vizwiz_answerability\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), save_dir / \"pytorch_model.bin\")\n",
    "processor.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model + processor saved to {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
